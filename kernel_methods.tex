% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[12pt]{article}

\usepackage{fontspec}
\usepackage{xeCJK}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}

\setCJKmainfont{LiHei Pro}
\XeTeXlinebreaklocale zh
\XeTeXlinebreakskip = 0pt plus 1pt

% ------ Thm. Def. etc. ---------

% Theorem Styles
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
% Definition Styles
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}

% ------ For pasting codes ------
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
% -----------------------------------

\begin{document}

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

\title{Tutorial: Kernel Methods}
\author{Chih-Wei Chang B00201037\\
Chung-Yen Hung B00201015}
\maketitle

\begin{abstract}
  The most common machine learning models are usually linear. Linear model possess some advantages such as simple, efficient, and computationally feasible. Real world data analysis problems, on the other hand, are rarely linear --- they usually require nonlinear models to describe the sophisticated interdependencies behind the data. One way to solve this problem is mapping features to higher dimensional space. After the mapping, we simply apply linear models in that space, which in turn give nonlinear models in the original lower dimensional space. However, mapping features to higher dimensional space also increase the computational cost, the inner product, for example. The higher the dimension, the harder the computation. Kernel methods, luckily, provide a mathematical shortcut for us to prevent calculating the inner product directly. In other words, kernel methods give us the ability to fit sophisticated models while still have the computational feasibility at the same time.
\end{abstract}

\section{Introduction to Kernel Method}

\subsection{Feature Transformation}
Feature transformation provides us a way to tackle on sophisticated data. Take the quadratic transformation from \(\mathbb{R}^2\) to \(\mathbb{R}^6\) as an instance, which can be defined by:
\[
  \Phi: (x_1, x_2) \mapsto (x_1^2, x_2^2, \sqrt{2} x_1 x_2, \sqrt{2} x_1, \sqrt{2} x_2, 1).
\]
Note that a straight line in the special space actually corresponds to a quadratic curve in the original space \(\mathbb{R}^2\). It implies that we can get a nonlinear model in the original space by fitting a linear model in \(\mathbb{R}^6\). In this way, any linear model can be extended to a nonlinear model. To be specific, we fit linear model on transformed data \(\Phi(\mathbf{x})\) and get a hypothesis \(h: \mathbb{R}^6 \rightarrow \{0, 1\} \). After that, when new testing data \(\mathbf{x}_{test}\) arriving,  \(h(\Phi(\mathbf{x}_{test}))\) gives the prediction.

\subsection{Kernel Trick}
While feature transformation seems promising --- the model can be extremely sophisticated as long as we make the dimension of the transformed space higher --- the price is the expensive computational cost. The computational difficulty also makes the infinity dimensional transformation impossible because we can't do a infinity dimensional inner product on a computer. Fortunately, kernel methods come to rescue.

Kernel method is a mathematical shortcut for preventing computing inner product directly. For example, consider the quadratic transformation mentioned previously, the inner product of \(\Phi(\mathbf{x})\) and \(\Phi(\mathbf{y})\) is
\begin{align*}
  \langle \Phi(\mathbf{x}), \Phi(\mathbf{y}) \rangle &= {\Phi(\mathbf{x})}^T \Phi(\mathbf{y}) \\
      &= x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 x_2 y_1 y_2 + 2 x_1 y_1 + 2 x_2 y_2 + 1 \\
      &= {(\mathbf{x}^T\mathbf{y} + 1)}^2 = {(\langle \mathbf{x}, \mathbf{y} \rangle + 1)}^2
\end{align*}
It implies that we can define the higher dimensional space's inner product by the simpler inner product in the original space, which successfully prevents the computational complexity. We hence define the kernel function, we will give a more rigorous definition later, by
\[
  K(\mathbf{x}_1, \mathbf{x_2}) = \langle \Phi(\mathbf{x_1}), \Phi(\mathbf{x_2}) \rangle
\]

As long as we can represent the transformed space inner product by original space inner product, we can have both the sophisticated hypothesis and computational feasibility. We then call this the ``Kernel Trick''.

\section{Common Kernel}
\subsection{Hilber Space}
\begin{definition}[Inner Product Space]
  An inner product space \(\mathcal{X}\) is a vector space with an associated inner product \(\langle \cdot,\cdot \rangle: \mathcal{X} \rightarrow \mathbb{R}\) that satisfies:
  \begin{itemize}
    \item Symmetry: \(\langle \mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{y}, \mathbf{x} \rangle\)
    \item Linearity: \(\langle a \cdot \mathbf{x}, \mathbf{y} \rangle = a \cdot \langle \mathbf{x}, \mathbf{y} \rangle\) and \(\langle \mathbf{w} + \mathbf{x}, \mathbf{y} \rangle =  \langle \mathbf{w}, \mathbf{y} \rangle + \langle \mathbf{x}, \mathbf{y} \rangle\)
    \item Positive Semi-Definiteness: \( \langle \mathbf{x}, \mathbf{x} \rangle \geq 0 \)
  \end{itemize}
  The inner product space is strict if \( \langle \mathbf{x}, \mathbf{x} \rangle = 0 \) iff \( \mathbf{x} = 0 \)
\end{definition}

\begin{definition}[Hilbert Space]
  A strict inner product space \(\mathcal{F}\) is a Hilbert space if it is:
  \begin{itemize}
    \item Complete: Every Cauchy sequence \(\{ h_i \in \mathcal{F} \}_{i=1}^\infty\) converges to an element \(h \in \mathcal{F}\)
    \item Separable: There is a countable subset \( \mathcal{\hat{F}} = \{ h_i \in \mathcal{F} \}_{i=1}^\infty \) such that for all \(h \in \mathcal{F}\) and \(\epsilon > 0 \), there exists \(h_i \in \mathcal{\hat{F}}\) such that \(\| h_i - h\| < \epsilon\).
  \end{itemize}
\end{definition}

The interval \([0, 1]\), the reals \(\mathbb{R}\), the complex numbers \(\mathbb{C}\) and Euclidean spaces \(\mathbb{R}^D\) are all Hilber spaces.

\subsection{Kernel Function}

\begin{definition}[Reproducing Kernel Function and RKHS]
  Let \(\mathcal{X}\) be an arbitrary set and \(F\) a Hilbert space over \(X\). The function \(K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\) is a reproducing kernel of \(\mathcal{F}\) if
  \begin{itemize}
    \item For every \(\mathbf{x} \in \mathcal{X}\), the function \(f_{\mathbf{x}}(\cdot) = K(\cdot, \mathbf{x})\) is in \(\mathcal{F}\).
    \item Reproducing Property: for every \(\mathbf{z} \in \mathcal{X}\) and every \(f \in \mathcal{F}\)
      \[
        f(\mathbf{z}) = {\langle f, K(\cdot, \mathbf{z}) \rangle}_{\mathcal{F}}
      \]
  \end{itemize}
  Further, the space is called a Reproducing Kernel Hilbert Space (RKHS).
\end{definition}

Note: why is reproducing property important?
Note: Maybe state and discuss Mercer's condition here.

\subsection{Polynomial Kernel}
The polynomial kernel is defined as
\[
  K_{\Phi_{d}}(x, y) = {(\langle x,y \rangle + \alpha)}^{d}
\]
As a kernel, K corresponds to an inner product in a feature space based on some mapping:
\[
  K_{\Phi_{d}}(x, y) = \langle \Phi_{d}(x), \Phi_{d}(y) \rangle
\]
Let \( d=2 \), so we get the special case of the quadratic kernel
\[
  K(x, y) = (\sum_{i=1}^{n} x_{i}y_{i} + \alpha)^{2} = \sum_{i=1}^{n}(x_{i}^{2})(y_{i}^{2}) + \sum_{i=2}^{n}\sum_{j=1}^{i-1}(\sqrt{2}x_{i}x_{j})(\sqrt{2}y_{i}y_{j}) + \sum_{i=1}^{n}(\sqrt{2c}x_{i})(\sqrt{2c}y_{i})
\]
From this it follows that the feature map is given by:
\[
  \Phi_{2}(x) = (x_{n}^{2},\dots,x_{1}^{2},\sqrt{2}x_{n}x_{n-1},\dots,\sqrt{2}x_{n}x_{1},\sqrt{2}x_{n-1}x_{n-2},\dots,\sqrt{2c}x_{n},\dots,\sqrt{2c}x_{1},c)
\]


\subsection{Gaussian Kernel}

\section{Kernel Machines}
\subsection{Kernel PCA}
\subsection{Kernel SVM}
\subsection{Kerenl Ridge Regression}
\subsection{Kernel Logistic Regression}

\section{Output Kernel}
\subsection{Kernel in Output Space}
\subsection{Principle Label Space Transform}
Just for temporal reference: Feature-aware Label Space Dimension Reduction for Multi-label Classification (2012) YN Chen and HT Lin.
\subsection{Conditional Principle Label Space Transform}
\subsection{Other Possible Output Kernel Techniques}

\section{Conclusion}
Here comes the Conclusion


\bibliographystyle{plain}
\bibliography{ref}


% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------

\end{document}

% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[12pt]{article}

\usepackage{fontspec}
\usepackage{xeCJK}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}

\setCJKmainfont{LiHei Pro}
\XeTeXlinebreaklocale zh
\XeTeXlinebreakskip = 0pt plus 1pt

% ------ Thm. Def. etc. ---------

% Theorem Styles
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
% Definition Styles
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}

% ------ For pasting codes ------
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
% -----------------------------------

\begin{document}

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

\title{Tutorial: Kernel Methods}
\author{Chih-Wei Chang B00201037\\
Chung-Yen Hung B00201015}
\maketitle

\begin{abstract}
  Here comes the Abstract
\end{abstract}

\section{Introduction to Kernel Method}
\subsection{What Is Kernel and Kernel Trick}
Kernel method is an assumption that using an transform so that a point set which is no linear separable in lower dimension can be linear separable in highter dimension. But there is a problem: compution is complex if we move those points to the high-dimension.\par
Kernel Trick is using kernel function $ K(x1,x2) = <\phi(x_{1}), \phi(x_{2})> $ where $x_{1}$ and $x_{2}$ are in the low-dimension. $ \phi$ is a transform for points from low-dimension to hight-dimension. Kernel trick can let compution be easy, because we just compute their kernel function inner product.
\subsection{Why Use Kernel and How Kernel Can Be Used}

\section{Common Kernel}
\subsection{Hilber Space and RKHS}
\begin{definition}[Inner Product Space]
  An inner product space \(\mathcal{X}\) is a vector space with an associated inner product \(\langle \cdot,\cdot \rangle: \mathcal{X} \rightarrow \mathbb{R}\) that satisfies:
  \begin{itemize}
    \item Symmetry: \(\langle \mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{y}, \mathbf{x} \rangle\)
    \item Linearity: \(\langle a \cdot \mathbf{x}, \mathbf{y} \rangle = a \cdot \langle \mathbf{x}, \mathbf{y} \rangle\) and \(\langle \mathbf{w} + \mathbf{x}, \mathbf{y} \rangle =  \langle \mathbf{w}, \mathbf{y} \rangle + \langle \mathbf{x}, \mathbf{y} \rangle\)
    \item Positive Semi-Definiteness: \( \langle \mathbf{x}, \mathbf{x} \rangle \geq 0 \)
  \end{itemize}
  The inner product space is strict if \( \langle \mathbf{x}, \mathbf{x} \rangle = 0 \) iff \( \mathbf{x} = 0 \)
\end{definition}

\begin{definition}[Hilbert Space]
  A strict inner product space \(\mathcal{F}\) is a Hilbert space if it is:
  \begin{itemize}
    \item Complete: Every Cauchy sequence \(\{ h_i \in \mathcal{F} \}_{i=1}^\infty\) converges to an element \(h \in \mathcal{F}\)
    \item Separable: There is a countable subset \( \mathcal{\hat{F}} = \{ h_i \in \mathcal{F} \}_{i=1}^\infty \) such that for all \(h \in \mathcal{F}\) and \(\epsilon > 0 \), there exists \(h_i \in \mathcal{\hat{F}}\) such that \(\| h_i - h\| < \epsilon\).
  \end{itemize}
\end{definition}

The interval \([0, 1]\), the reals \(\mathbb{R}\), the complex numbers \(\mathbb{C}\) and Euclidean spaces \(\mathbb{R}^D\) are all Hilber spaces.
\subsection{Kerenl Function}
\subsection{Polynomial Kernel}
The polynomial kernel is defined as
\[
  K_{\Phi_{d}}(x, y) = {(\langle x,y \rangle + \alpha)}^{d}
\]
As a kernel, K corresponds to an inner product in a feature space based on some mapping:
\[
  K_{\Phi_{d}}(x, y) = \langle \Phi_{d}(x), \Phi_{d}(y) \rangle
\]
Let \( d=2 \), so we get the special case of the quadratic kernel
\[
  K(x, y) = (\sum_{i=1}^{n} x_{i}y_{i} + \alpha)^{2} = \sum_{i=1}^{n}(x_{i}^{2})(y_{i}^{2}) + \sum_{i=2}^{n}\sum_{j=1}^{i-1}(\sqrt{2}x_{i}x_{j})(\sqrt{2}y_{i}y_{j}) + \sum_{i=1}^{n}(\sqrt{2c}x_{i})(\sqrt{2c}y_{i})
\]
From this it follows that the feature map is given by:
\[
\Phi_{2}(x) = \langle x_{n}^{2},\dots,x_{1}^{2},\sqrt{2}x_{n}x_{n-1},\dots,\sqrt{2}x_{n}x_{1},\sqrt{2}x_{n-1}x_{n-2},\dots,\sqrt{2c}x_{n},\dots,\sqrt{2c}x_{1},c \rangle
\]


\subsection{Gaussian Kernel}

\section{Kernel Machines}
\subsection{Kernel PCA}
\subsection{Kernel SVM}
\subsection{Kerenl Ridge Regression}
\subsection{Kernel Logistic Regression}

\section{Output Kernel}
\subsection{Kernel in Output Space}
\subsection{Principle Label Space Transform}
Just for temporal reference: Feature-aware Label Space Dimension Reduction for Multi-label Classification (2012) YN Chen and HT Lin.
\subsection{Conditional Principle Label Space Transform}
\subsection{Other Possible Output Kernel Techniques}

\section{Conclusion}
Here comes the Conclusion


\bibliographystyle{plain}
\bibliography{ref}


% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------

\end{document}
